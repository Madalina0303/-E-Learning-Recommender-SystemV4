# -*- coding: utf-8 -*-
"""QuantumExperiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TV6Q9tGmqZ-9ZfezMv7MXDmk9E8c3-Gl
"""

pip install pennylane torch

import pandas as pd
import numpy as np
import time
from datetime import datetime
import pennylane as qml
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

NUM_RUNS = 5
PREPROCESSED_FILE = "/content/drive/MyDrive/QuantumElearningDataSet/processed_summaries_q_s.csv"
MAIN_DATA_FILE = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"
OUTPUT_FILE = "/content/drive/MyDrive/Quantum-LogFiles/15_cos_similarity_results.csv"
LOG_FILE = "/content/drive/MyDrive/Quantum-LogFiles/15_cos_similarity_logs.txt"
N_QUBITS = 15

def quantum_feature_map(x, run_id):
    n_qubits = len(x)
    dev = qml.device("default.qubit", wires=n_qubits)
    @qml.qnode(dev)
    def circuit():
        for i, feature in enumerate(x):
            qml.RY(feature, wires=i)
        weights = np.random.random(size=(1, n_qubits))
        qml.templates.BasicEntanglerLayers(weights=weights, wires=range(n_qubits))
        return qml.state()
    return circuit()

def quantum_similarity(vectors, run_id):
    quantum_states = [quantum_feature_map(vec, run_id) for vec in vectors]
    quantum_real_parts = [np.real(state) for state in quantum_states]
    similarities = cosine_similarity(quantum_real_parts)
    return similarities

def preprocess_data():

    df_processed = pd.read_csv(PREPROCESSED_FILE)

    vectorizer = TfidfVectorizer(max_features=1000)
    tfidf_matrix = vectorizer.fit_transform(df_processed["Processed_Summary"]).toarray()

    svd = TruncatedSVD(n_components=N_QUBITS)
    reduced_matrix = svd.fit_transform(tfidf_matrix)

    return reduced_matrix

def run_single_iteration(run_id, df, reduced_matrix):
    start_time = time.time()

    similarity_matrix = quantum_similarity(reduced_matrix, run_id)

    metrics = {
        'run_id': run_id,
        'nr_diferente': 0,
        'total_corecte': 0,
        'perfect_matches': 0,
        'at_least_one_mismatch': 0,
        'accuracies': []
    }

    for i, row in df.iterrows():
        similarities = similarity_matrix[i]
        top_5_indices = np.argsort(similarities)[-6:-1][::-1]
        current_category = df.iloc[i]["Category"]
        nr_corecte = sum(df.iloc[idx]["Category"] == current_category for idx in top_5_indices)

        metrics['accuracies'].append((nr_corecte / 5) * 100)
        metrics['total_corecte'] += nr_corecte
        metrics['nr_diferente'] += (5 - nr_corecte)

        if nr_corecte == 5:
            metrics['perfect_matches'] += 1
        else:
            metrics['at_least_one_mismatch'] += 1

    total_predicted = len(df) * 5
    metrics.update({
        'correct_percentage': (metrics['total_corecte'] / total_predicted) * 100,
        'different_percentage': (metrics['nr_diferente'] / total_predicted) * 100,
        'perfect_match_percentage': (metrics['perfect_matches'] / len(df)) * 100,
        'mismatch_percentage': (metrics['at_least_one_mismatch'] / len(df)) * 100,
        'mean_accuracy': np.mean(metrics['accuracies']),
        'execution_time': time.time() - start_time
    })

    return metrics

with open(LOG_FILE, "w") as f:
    f.write(f"Quantum Similarity Experiment - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Configuration: {NUM_RUNS} runs, {N_QUBITS} qubits\n\n")

df_main = pd.read_csv(MAIN_DATA_FILE)
reduced_matrix = preprocess_data()
all_metrics = []

for run in range(1, NUM_RUNS+1):
    print(f"\n{'='*40}\nRun {run}/{NUM_RUNS}\n{'='*40}")

    metrics = run_single_iteration(run, df_main, reduced_matrix)
    all_metrics.append(metrics)

    log_entry = f"""Run {run} Results:
âœ… Correct: {metrics['total_corecte']} ({metrics['correct_percentage']:.2f}%)
âŒ Wrong: {metrics['nr_diferente']} ({metrics['different_percentage']:.2f}%)
ðŸŽ¯ Perfect: {metrics['perfect_matches']} ({metrics['perfect_match_percentage']:.2f}%)
âš ï¸ Mismatch: {metrics['at_least_one_mismatch']} ({metrics['mismatch_percentage']:.2f}%)
ðŸ“Š Accuracy: {metrics['mean_accuracy']:.2f}%
â±ï¸ Time: {metrics['execution_time']:.2f}s\n"""

    print(log_entry)
    with open(LOG_FILE, "a") as f:
        f.write(log_entry)

if NUM_RUNS > 1:
    avg_metrics = {
        'correct_percentage': np.mean([m['correct_percentage'] for m in all_metrics]),
        'different_percentage': np.mean([m['different_percentage'] for m in all_metrics]),
        'perfect_match_percentage': np.mean([m['perfect_match_percentage'] for m in all_metrics]),
        'mismatch_percentage': np.mean([m['mismatch_percentage'] for m in all_metrics]),
        'mean_accuracy': np.mean([m['mean_accuracy'] for m in all_metrics]),
        'execution_time': np.mean([m['execution_time'] for m in all_metrics])
    }

    summary_log = f"""\n{'='*40}
FINAL SUMMARY AFTER {NUM_RUNS} RUNS
Average correct: {avg_metrics['correct_percentage']:.2f}%
Average wrong: {avg_metrics['different_percentage']:.2f}%
Average perfect: {avg_metrics['perfect_match_percentage']:.2f}%
Average mismatch: {avg_metrics['mismatch_percentage']:.2f}%
Overall accuracy: {avg_metrics['mean_accuracy']:.2f}%
Average time: {avg_metrics['execution_time']:.2f}s\n"""

    print(summary_log)
    with open(LOG_FILE, "a") as f:
        f.write(summary_log)

results_df = pd.DataFrame(all_metrics)
# results_df.to_csv(OUTPUT_FILE, index=False)
# print(f"\nðŸ“Š Results saved to: {OUTPUT_FILE}")
print(f"ðŸ“„ Full log saved to: {LOG_FILE}")

"""Method2-QUANTUM FIDELITY"""

import pandas as pd
import numpy as np
import time
from datetime import datetime
import pennylane as qml
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

NUM_RUNS = 5
PREPROCESSED_FILE = "/content/drive/MyDrive/QuantumElearningDataSet/processed_summaries_q_s.csv"
MAIN_DATA_FILE = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"
LOG_FILE = "/content/drive/MyDrive/Quantum-LogFiles/8_quanutum_fidelity_logs.txt_v2"
N_QUBITS = 8

def quantum_feature_map(x):
    n_qubits = len(x)
    dev = qml.device("default.qubit", wires=n_qubits)
    @qml.qnode(dev)
    def circuit():
        for i, feature in enumerate(x):
            qml.RY(feature, wires=i)
        weights = np.random.random(size=(1, n_qubits))
        qml.templates.BasicEntanglerLayers(weights=weights, wires=range(n_qubits))
        return qml.state()
    return circuit()

def quantum_similarity(vectors):
    quantum_states = np.array([quantum_feature_map(vec) for vec in vectors])
    density_matrices = qml.math.dm_from_state_vector(quantum_states)  # Vectorizat

    n = len(quantum_states)
    fidelities = np.zeros((n, n))
    for i in range(n):
        for j in range(i, n):
            fidelities[i][j] = qml.math.fidelity(density_matrices[i], density_matrices[j])
            fidelities[j][i] = fidelities[i][j]  # Simetrie
    return fidelities

# def quantum_similarity(vectors):
#     quantum_states = [quantum_feature_map(vec) for vec in vectors]
#     fidelities = np.zeros((len(quantum_states), len(quantum_states)))

#     for i in range(len(quantum_states)):
#         for j in range(len(quantum_states)):
#             if i<=j:
#               # Convert the quantum state vectors to density matrices
#               state_i = qml.math.dm_from_state_vector(quantum_states[i])
#               state_j = qml.math.dm_from_state_vector(quantum_states[j])

#               # Calculate the fidelity between the two states
#               fidelity = qml.math.fidelity(state_i, state_j)
#               fidelities[i][j] = fidelity
#               fidelities[j][i] = fidelity
#     return fidelities

def preprocess_data():
    df_processed = pd.read_csv(PREPROCESSED_FILE)

    vectorizer = TfidfVectorizer(max_features=1000)
    tfidf_matrix = vectorizer.fit_transform(df_processed["Processed_Summary"]).toarray()

    svd = TruncatedSVD(n_components=N_QUBITS)
    reduced_matrix = svd.fit_transform(tfidf_matrix)

    return reduced_matrix

def run_single_iteration(run_id, df, reduced_matrix):
    start_time = time.time()

    # Generare matrice de similaritate cuanticÄƒ
    similarity_matrix = quantum_similarity(reduced_matrix)

    metrics = {
        'run_id': run_id,
        'nr_diferente': 0,
        'total_corecte': 0,
        'perfect_matches': 0,
        'at_least_one_mismatch': 0,
        'accuracies': []
    }

    for i, row in df.iterrows():
        similarities = similarity_matrix[i]
        top_5_indices = np.argsort(similarities)[-6:-1][::-1]
        current_category = df.iloc[i]["Category"]
        nr_corecte = sum(df.iloc[idx]["Category"] == current_category for idx in top_5_indices)

        metrics['accuracies'].append((nr_corecte / 5) * 100)
        metrics['total_corecte'] += nr_corecte
        metrics['nr_diferente'] += (5 - nr_corecte)

        if nr_corecte == 5:
            metrics['perfect_matches'] += 1
        else:
            metrics['at_least_one_mismatch'] += 1

    total_predicted = len(df) * 5
    metrics.update({
        'correct_percentage': (metrics['total_corecte'] / total_predicted) * 100,
        'different_percentage': (metrics['nr_diferente'] / total_predicted) * 100,
        'perfect_match_percentage': (metrics['perfect_matches'] / len(df)) * 100,
        'mismatch_percentage': (metrics['at_least_one_mismatch'] / len(df)) * 100,
        'mean_accuracy': np.mean(metrics['accuracies']),
        'execution_time': time.time() - start_time
    })

    return metrics

with open(LOG_FILE, "w") as f:
    f.write(f"Quantum Similarity Experiment - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Configuration: {NUM_RUNS} runs, {N_QUBITS} qubits\n\n")

df_main = pd.read_csv(MAIN_DATA_FILE)
reduced_matrix = preprocess_data()
all_metrics = []

for run in range(1, NUM_RUNS+1):
    print(f"\n{'='*40}\nRun {run}/{NUM_RUNS}\n{'='*40}")

    metrics = run_single_iteration(run, df_main, reduced_matrix)
    all_metrics.append(metrics)


    log_entry = f"""Run {run} Results:
âœ… Correct: {metrics['total_corecte']} ({metrics['correct_percentage']:.2f}%)
âŒ Wrong: {metrics['nr_diferente']} ({metrics['different_percentage']:.2f}%)
ðŸŽ¯ Perfect: {metrics['perfect_matches']} ({metrics['perfect_match_percentage']:.2f}%)
âš ï¸ Mismatch: {metrics['at_least_one_mismatch']} ({metrics['mismatch_percentage']:.2f}%)
ðŸ“Š Accuracy: {metrics['mean_accuracy']:.2f}%
â±ï¸ Time: {metrics['execution_time']:.2f}s\n"""

    print(log_entry)
    with open(LOG_FILE, "a") as f:
        f.write(log_entry)

if NUM_RUNS > 1:
    avg_metrics = {
        'correct_percentage': np.mean([m['correct_percentage'] for m in all_metrics]),
        'different_percentage': np.mean([m['different_percentage'] for m in all_metrics]),
        'perfect_match_percentage': np.mean([m['perfect_match_percentage'] for m in all_metrics]),
        'mismatch_percentage': np.mean([m['mismatch_percentage'] for m in all_metrics]),
        'mean_accuracy': np.mean([m['mean_accuracy'] for m in all_metrics]),
        'execution_time': np.mean([m['execution_time'] for m in all_metrics])
    }

    summary_log = f"""\n{'='*40}
FINAL SUMMARY AFTER {NUM_RUNS} RUNS
Average correct: {avg_metrics['correct_percentage']:.2f}%
Average wrong: {avg_metrics['different_percentage']:.2f}%
Average perfect: {avg_metrics['perfect_match_percentage']:.2f}%
Average mismatch: {avg_metrics['mismatch_percentage']:.2f}%
Overall accuracy: {avg_metrics['mean_accuracy']:.2f}%
Average time: {avg_metrics['execution_time']:.2f}s\n"""

    print(summary_log)
    with open(LOG_FILE, "a") as f:
        f.write(summary_log)

results_df = pd.DataFrame(all_metrics)
# results_df.to_csv(OUTPUT_FILE, index=False)
# print(f"\nðŸ“Š Results saved to: {OUTPUT_FILE}")
print(f"ðŸ“„ Full log saved to: {LOG_FILE}")

"""METOD3- QML fixed weights"""

import pandas as pd
import numpy as np
import time
from datetime import datetime
import pennylane as qml
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

NUM_RUNS = 5
PREPROCESSED_FILE = "/content/drive/MyDrive/QuantumElearningDataSet/processed_summaries_q_s.csv"
MAIN_DATA_FILE = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"
LOG_FILE = "/content/drive/MyDrive/Quantum-LogFiles/6_quanutum_fidelity_ponderi_fixe_logs.txt"
N_QUBITS = 6


weights = np.random.random(size=(1, 6))
def quantum_feature_map(x):
    n_qubits = len(x)
    dev = qml.device("default.qubit", wires=n_qubits)
    @qml.qnode(dev)
    def circuit():
        for i, feature in enumerate(x):
            qml.RY(feature, wires=i)
        # weights = np.random.random(size=(1, n_qubits))
        qml.templates.BasicEntanglerLayers(weights=weights, wires=range(n_qubits))
        return qml.state()
    return circuit()

def quantum_similarity(vectors):
    quantum_states = np.array([quantum_feature_map(vec) for vec in vectors])
    density_matrices = qml.math.dm_from_state_vector(quantum_states)

    n = len(quantum_states)
    fidelities = np.zeros((n, n))
    for i in range(n):
        for j in range(i, n):
            fidelities[i][j] = qml.math.fidelity(density_matrices[i], density_matrices[j])
            fidelities[j][i] = fidelities[i][j]
    return fidelities

# def quantum_similarity(vectors):
#     quantum_states = [quantum_feature_map(vec) for vec in vectors]
#     fidelities = np.zeros((len(quantum_states), len(quantum_states)))

#     for i in range(len(quantum_states)):
#         for j in range(len(quantum_states)):
#             if i<=j:
#               # Convert the quantum state vectors to density matrices
#               state_i = qml.math.dm_from_state_vector(quantum_states[i])
#               state_j = qml.math.dm_from_state_vector(quantum_states[j])

#               # Calculate the fidelity between the two states
#               fidelity = qml.math.fidelity(state_i, state_j)
#               fidelities[i][j] = fidelity
#               fidelities[j][i] = fidelity
#     return fidelities

def preprocess_data():
    df_processed = pd.read_csv(PREPROCESSED_FILE)

    vectorizer = TfidfVectorizer(max_features=1000)
    tfidf_matrix = vectorizer.fit_transform(df_processed["Processed_Summary"]).toarray()

    svd = TruncatedSVD(n_components=N_QUBITS)
    reduced_matrix = svd.fit_transform(tfidf_matrix)

    return reduced_matrix

def run_single_iteration(run_id, df, reduced_matrix):
    start_time = time.time()

    similarity_matrix = quantum_similarity(reduced_matrix)

    metrics = {
        'run_id': run_id,
        'nr_diferente': 0,
        'total_corecte': 0,
        'perfect_matches': 0,
        'at_least_one_mismatch': 0,
        'accuracies': []
    }

    for i, row in df.iterrows():
        similarities = similarity_matrix[i]
        top_5_indices = np.argsort(similarities)[-6:-1][::-1]
        current_category = df.iloc[i]["Category"]
        nr_corecte = sum(df.iloc[idx]["Category"] == current_category for idx in top_5_indices)

        metrics['accuracies'].append((nr_corecte / 5) * 100)
        metrics['total_corecte'] += nr_corecte
        metrics['nr_diferente'] += (5 - nr_corecte)

        if nr_corecte == 5:
            metrics['perfect_matches'] += 1
        else:
            metrics['at_least_one_mismatch'] += 1

    total_predicted = len(df) * 5
    metrics.update({
        'correct_percentage': (metrics['total_corecte'] / total_predicted) * 100,
        'different_percentage': (metrics['nr_diferente'] / total_predicted) * 100,
        'perfect_match_percentage': (metrics['perfect_matches'] / len(df)) * 100,
        'mismatch_percentage': (metrics['at_least_one_mismatch'] / len(df)) * 100,
        'mean_accuracy': np.mean(metrics['accuracies']),
        'execution_time': time.time() - start_time
    })

    return metrics

with open(LOG_FILE, "w") as f:
    f.write(f"Quantum Similarity Experiment - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Configuration: {NUM_RUNS} runs, {N_QUBITS} qubits\n\n")

df_main = pd.read_csv(MAIN_DATA_FILE)
reduced_matrix = preprocess_data()
all_metrics = []


for run in range(1, NUM_RUNS+1):
    print(f"\n{'='*40}\nRun {run}/{NUM_RUNS}\n{'='*40}")

    metrics = run_single_iteration(run, df_main, reduced_matrix)
    all_metrics.append(metrics)

    log_entry = f"""Run {run} Results:
âœ… Correct: {metrics['total_corecte']} ({metrics['correct_percentage']:.2f}%)
âŒ Wrong: {metrics['nr_diferente']} ({metrics['different_percentage']:.2f}%)
ðŸŽ¯ Perfect: {metrics['perfect_matches']} ({metrics['perfect_match_percentage']:.2f}%)
âš ï¸ Mismatch: {metrics['at_least_one_mismatch']} ({metrics['mismatch_percentage']:.2f}%)
ðŸ“Š Accuracy: {metrics['mean_accuracy']:.2f}%
â±ï¸ Time: {metrics['execution_time']:.2f}s\n"""

    print(log_entry)
    with open(LOG_FILE, "a") as f:
        f.write(log_entry)

if NUM_RUNS > 1:
    avg_metrics = {
        'correct_percentage': np.mean([m['correct_percentage'] for m in all_metrics]),
        'different_percentage': np.mean([m['different_percentage'] for m in all_metrics]),
        'perfect_match_percentage': np.mean([m['perfect_match_percentage'] for m in all_metrics]),
        'mismatch_percentage': np.mean([m['mismatch_percentage'] for m in all_metrics]),
        'mean_accuracy': np.mean([m['mean_accuracy'] for m in all_metrics]),
        'execution_time': np.mean([m['execution_time'] for m in all_metrics])
    }

    summary_log = f"""\n{'='*40}
FINAL SUMMARY AFTER {NUM_RUNS} RUNS
Average correct: {avg_metrics['correct_percentage']:.2f}%
Average wrong: {avg_metrics['different_percentage']:.2f}%
Average perfect: {avg_metrics['perfect_match_percentage']:.2f}%
Average mismatch: {avg_metrics['mismatch_percentage']:.2f}%
Overall accuracy: {avg_metrics['mean_accuracy']:.2f}%
Average time: {avg_metrics['execution_time']:.2f}s\n"""

    print(summary_log)
    with open(LOG_FILE, "a") as f:
        f.write(summary_log)

results_df = pd.DataFrame(all_metrics)
# results_df.to_csv(OUTPUT_FILE, index=False)
# print(f"\nðŸ“Š Results saved to: {OUTPUT_FILE}")
print(f"ðŸ“„ Full log saved to: {LOG_FILE}")