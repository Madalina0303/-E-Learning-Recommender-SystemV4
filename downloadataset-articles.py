# -*- coding: utf-8 -*-
"""DownloaDataSet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vNd7PfXVNQfjBF7sPlcIzhOZQoSGSnKm
"""

import requests
import xml.etree.ElementTree as ET
import csv

def download_full_content(link):
    try:
        response = requests.get(link)
        if response.status_code == 200:
            return response.text
        else:
            return "Failed to download"
    except Exception as e:
        return f"Error: {str(e)}"

def fetch_arxiv_data():
    url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": "all:quantum computing",
        "start": 0,
        "max_results": 500
    }
    response = requests.get(url, params=params)


    if response.status_code != 200:
        print(f"Eroare la API: {response.status_code}")
        return

    root = ET.fromstring(response.text)
    ns = {'arxiv': 'http://www.w3.org/2005/Atom'}


    dataset = []

    for i, entry in enumerate(root.findall('arxiv:entry', ns)):
        title = entry.find('arxiv:title', ns).text.strip()
        authors = ", ".join(author.find('arxiv:name', ns).text for author in entry.findall('arxiv:author', ns))
        published = entry.find('arxiv:published', ns).text
        summary = entry.find('arxiv:summary', ns).text.strip()
        content_link = entry.find('arxiv:id', ns).text


        full_content = download_full_content(content_link)


        dataset.append({
            "Index": i + 1,
            "Title": title,
            "Authors": authors,
            "Published": published,
            "Summary": summary,
            "Content": full_content
        })


    output_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing.csv"
    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=["Index", "Title", "Authors", "Published", "Summary", "Content"])
        writer.writeheader()
        writer.writerows(dataset)

    print(f"Datele au fost salvate în {output_file}")


fetch_arxiv_data()

pip install PyPDF2

import requests
import xml.etree.ElementTree as ET
import csv
import io
from PyPDF2 import PdfReader

def download_pdf_and_extract_text(pdf_url):
    try:
        response = requests.get(pdf_url)
        if response.status_code == 200:
            pdf_file = io.BytesIO(response.content)
            pdf_reader = PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text.strip() if text else "No text extracted"
        else:
            return "Failed to download PDF"
    except Exception as e:
        return f"Error: {str(e)}"

def fetch_arxiv_data():
    url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": "all:quantum computing",
        "start": 0,
        "max_results": 500
    }
    response = requests.get(url, params=params)

    if response.status_code != 200:
        print(f"Eroare la API: {response.status_code}")
        return

    root = ET.fromstring(response.text)
    ns = {'arxiv': 'http://www.w3.org/2005/Atom'}

    dataset = []

    for i, entry in enumerate(root.findall('arxiv:entry', ns)):
        title = entry.find('arxiv:title', ns).text.strip()
        authors = ", ".join(author.find('arxiv:name', ns).text for author in entry.findall('arxiv:author', ns))
        published = entry.find('arxiv:published', ns).text
        summary = entry.find('arxiv:summary', ns).text.strip()

        pdf_link = entry.find('.//arxiv:link[@title="pdf"]', ns)
        pdf_url = pdf_link.attrib['href'] if pdf_link is not None else None

        pdf_text = download_pdf_and_extract_text(pdf_url) if pdf_url else "No PDF available"

        dataset.append({
            "Index": i + 1,
            "Title": title,
            "Authors": authors,
            "Published": published,
            "Summary": summary,
            "PDF_Content": pdf_text
        })

    output_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"
    # with open(output_file, mode='w', newline='', encoding='utf-8') as file:
    #     writer = csv.DictWriter(file, fieldnames=["Index", "Title", "Authors", "Published", "Summary", "PDF_Content"])
    #     writer.writeheader()
    #     writer.writerows(dataset)
    with open(output_file, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(
        file,
        fieldnames=["Index", "Title", "Authors", "Published", "Summary", "PDF_Content"],
        escapechar='\\',
        quoting=csv.QUOTE_MINIMAL
        )
        writer.writeheader()
        writer.writerows(dataset)
    print(f"Datele au fost salvate în {output_file}")

fetch_arxiv_data()

import csv
import sys

csv.field_size_limit(sys.maxsize)

csv_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

def count_entries_in_csv(file_name):
    try:
        with open(file_name, mode='r', encoding='utf-8') as file:
            content = file.read().replace('\x00', '')  # Eliminăm caracterele NUL
            lines = content.splitlines()
            reader = csv.DictReader(lines)
            entries = list(reader)
            print(f"Numărul de intrări în fișierul CSV {len(entries)}")
    except FileNotFoundError:
        print(f"Fișierul {file_name} nu a fost găsit.")
    except Exception as e:
        print(f"A apărut o eroare: {e}")

count_entries_in_csv(csv_file)

import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

nltk.download("stopwords")
nltk.download("punkt")
nltk.download('punkt_tab')
nlp = spacy.load("en_core_web_sm")

def preprocess_text(text):
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)

    text = re.sub(r'\d+', '', text)

    words = word_tokenize(text.lower())  # tokenize and convert to lowercase

    # Remove stop words
    stop_words = set(stopwords.words("english"))
    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]

    # Stemming
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in filtered_words]

    # Lemmatize with spaCy
    doc = nlp(" ".join(stemmed_words))
    lemmatized_words = [token.lemma_ for token in doc]

    # Return the processed text
    return " ".join(lemmatized_words)

# Read the CSV file
input_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

df = pd.read_csv(input_file, usecols=["Index", "Summary"])

processed_data = {}
for index, row in df.iterrows():
    preprocessed_summary = preprocess_text(row["Summary"])
    processed_data[row["Index"]] = preprocessed_summary

for index, row in df.iterrows():
    preprocessed_summary = preprocess_text(row["Summary"])
    processed_data[row["Index"]] = preprocessed_summary

# Convert processed data into a DataFrame
processed_df = pd.DataFrame(list(processed_data.items()), columns=["Index", "Processed_Summary"])

# Save the processed data to a new CSV
processed_df.to_csv("/content/drive/MyDrive/QuantumElearningDataSet/processed_summaries.csv", index=False)

print("Processed summaries saved to 'processed_summaries.csv'")

import pandas as pd

csv_path = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

df = pd.read_csv(csv_path)

df["category"] = "quantum"

df.to_csv(csv_path, index=False)

"""SOCIAL SCIENCE ARTICLES"""

import requests
import xml.etree.ElementTree as ET
import pandas as pd
import io
from PyPDF2 import PdfReader

csv_path = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

def download_pdf_and_extract_text(pdf_url):
    try:
        response = requests.get(pdf_url)
        if response.status_code == 200:
            pdf_file = io.BytesIO(response.content)
            pdf_reader = PdfReader(pdf_file)
            text = "\n".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])
            return text.strip() if text else "No text extracted"
        else:
            return "Failed to download PDF"
    except Exception as e:
        return f"Error: {str(e)}"

def fetch_arxiv_data():
    url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": "all:social science",
        "start": 0,
        "max_results": 24
    }

    response = requests.get(url, params=params)
    if response.status_code != 200:
        print(f"Eroare la API: {response.status_code}")
        return

    root = ET.fromstring(response.text)
    ns = {'arxiv': 'http://www.w3.org/2005/Atom'}

    dataset = []

    for i, entry in enumerate(root.findall('arxiv:entry', ns)):
        title = entry.find('arxiv:title', ns).text.strip()
        authors = ", ".join(author.find('arxiv:name', ns).text for author in entry.findall('arxiv:author', ns))
        published = entry.find('arxiv:published', ns).text
        summary = entry.find('arxiv:summary', ns).text.strip()

        pdf_link = entry.find('.//arxiv:link[@title="pdf"]', ns)
        pdf_url = pdf_link.attrib['href'] if pdf_link is not None else None

        pdf_text = download_pdf_and_extract_text(pdf_url) if pdf_url else "No PDF available"

        dataset.append({
            "Index": 467 + i,
            "Title": title,
            "Authors": authors,
            "Published": published,
            "Summary": summary,
            "PDF_Content": pdf_text,
            "Category": "social science"
        })

    return dataset

new_data = fetch_arxiv_data()

df_new = pd.DataFrame(new_data)

df_existing = pd.read_csv(csv_path)

df_final = pd.concat([df_existing, df_new], ignore_index=True)

df_final.to_csv(csv_path, index=False)

print("24 articole despre social science au fost adăugate cu succes!")

import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re

nltk.download("stopwords")
nltk.download("punkt")
nltk.download('punkt_tab')
nlp = spacy.load("en_core_web_sm")
def preprocess_text(text):
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)

    text = re.sub(r'\d+', '', text)

    words = word_tokenize(text.lower())  # tokenize and convert to lowercase

    # Remove stop words
    stop_words = set(stopwords.words("english"))
    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]

    # Stemming
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in filtered_words]

    # Lemmatize with spaCy
    doc = nlp(" ".join(stemmed_words))
    lemmatized_words = [token.lemma_ for token in doc]

    return " ".join(lemmatized_words)

input_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

df = pd.read_csv(input_file, usecols=["Index", "Summary"])

processed_data = {}
for index, row in df.iterrows():
    preprocessed_summary = preprocess_text(row["Summary"])
    processed_data[row["Index"]] = preprocessed_summary

for index, row in df.iterrows():
    preprocessed_summary = preprocess_text(row["Summary"])
    processed_data[row["Index"]] = preprocessed_summary

processed_df = pd.DataFrame(list(processed_data.items()), columns=["Index", "Processed_Summary"])

# Save the processed data to a new CSV
processed_df.to_csv("/content/drive/MyDrive/QuantumElearningDataSet/processed_summaries_q_s.csv", index=False)

print("Processed summaries saved to 'processed_summaries_q_s.csv'")

import pandas as pd

input_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

df = pd.read_csv(input_file, usecols=["Category"])

print(df)

import pandas as pd

input_file = "/content/drive/MyDrive/QuantumElearningDataSet/arxiv_quantum_computing_pdf.csv"

# Citim fișierul
df = pd.read_csv(input_file)

# Combinăm cele două coloane fără să pierdem date
df["Category"] = df["Category"].fillna(df["category"]) if "category" in df.columns else df["Category"]
df["Category"] = df["category"].fillna(df["Category"]) if "Category" in df.columns else df["category"]

# Ștergem coloana redundantă (dacă există)
df = df.drop(columns=["category"], errors="ignore")

# Salvăm modificările (opțional)
df.to_csv(input_file, index=False)

# Verificăm primele rânduri
print(df[["Category"]].head())